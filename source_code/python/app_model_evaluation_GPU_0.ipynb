{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # second gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from helpers import pre_processing_wlan_utils as preprocess_utils\n",
    "from helpers import classifier_wlan_spectral_utils as classifier_utils\n",
    "from helpers import tr_models as tr_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'helpers.pre_processing_wlan_utils' from '/project/traffic-recognition-2020/source_code/python/helpers/pre_processing_wlan_utils.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(tr_models)\n",
    "importlib.reload(classifier_utils)\n",
    "importlib.reload(preprocess_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label id:  3\n",
      "Num classes in that label:  7\n",
      "Labels:  ['spotify', 'tunein', 'gpodcast', 'youtube', 'netflix', 'twitch', 'no-app']\n",
      "[0 1]\n",
      "[5000]\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "task = \"app\"\n",
    "label = preprocess_utils.label_index[task]\n",
    "num_classes = preprocess_utils.num_classes[task]\n",
    "labels_string = preprocess_utils.labels_string[task]\n",
    "\n",
    "#adapting for 7 labels\n",
    "num_classes = num_classes-1\n",
    "del labels_string[7]\n",
    "print(\"Label id: \", label)\n",
    "print(\"Num classes in that label: \", num_classes)\n",
    "print(\"Labels: \", labels_string)\n",
    "\n",
    "padding = 'post'\n",
    "num_repetitions = np.arange(0,2)\n",
    "print(num_repetitions)\n",
    "now = datetime.now()\n",
    "datenow = now.strftime('%d%m%y%H%M%S')\n",
    "show_model = True\n",
    "\n",
    "max_iq_samples = [5000]#[100, 300, 500, 700, 1000, 2000]\n",
    "print(max_iq_samples)\n",
    "\n",
    "models_parameters = {}\n",
    "models_to_train = ['CNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for CNN\n",
    "model_param={}\n",
    "model_param['max_epochs']=100\n",
    "model_param['kernel_size']=32\n",
    "model_param['n_filters']=64\n",
    "model_param['dropout']=0.1\n",
    "model_param['batch_size']=64\n",
    "models_parameters[models_to_train[0]]=model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../../dataset/waveforms/'\n",
    "dataset_filename = 'waveforms_SNR_v2_16042020_2G_n_mobile_7_classes_app_balanced.mat'\n",
    "#dataset_filename = 'waveforms_07082020_2G_n_mobile_L8_SNR_app_balanced.mat'\n",
    "#dataset_filename = 'waveforms_16042020_2G_n_mobile_7_classes_app_balanced.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71603/71603 [03:47<00:00, 315.13it/s]\n"
     ]
    }
   ],
   "source": [
    "Xraw, Yraw = classifier_utils.get_raw_xy_spectrum(dataset_folder,dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1188/71603 [00:00<00:05, 11877.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding/Truncating sequence  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71603/71603 [00:06<00:00, 11813.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oneshot labels\n",
      "Labels created\n",
      "Performing data splitting\n",
      "Starting first partitioning\n",
      "First partitioning done. Starting partitioning 2\n",
      "Final partitioning done\n",
      "(57282, 2, 5000) (7160, 2, 5000) (7161, 2, 5000)\n",
      "This is experiment:  0  with  5000\n",
      "Starting data preparation and training for model  CNN\n",
      "Re-shaping for CNN\n",
      "(57282, 2, 5000, 1) (7160, 2, 5000, 1) (7161, 2, 5000, 1)\n",
      "Creating the model\n",
      "Model type CNN\n",
      "Model parameters: kernel size  32 , number of filter  64 , and dropout  0.1\n",
      "Model created\n",
      "Model start training\n",
      "Training for  100  epochs and batch size  64\n",
      "Epoch 1/100\n",
      "222/896 [======>.......................] - ETA: 6:47 - loss: 1.7005 - accuracy: 0.3223"
     ]
    }
   ],
   "source": [
    "experiment_results = {}\n",
    "\n",
    "for num_iq_seq in max_iq_samples:\n",
    "    \n",
    "    prefix = str(datenow)+\"_app\"+\"_iq_samples_\"+str(num_iq_seq)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    print(\"Padding/Truncating sequence \",str(num_iq_seq))\n",
    "    X = classifier_utils.pad_or_trunc_x_and_scale(Xraw, num_iq_seq, padding, scale=False)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    print(\"Oneshot labels\")\n",
    "    Y = classifier_utils.get_one_hot_labels(Yraw, num_classes, label)\n",
    "    \n",
    "    print(\"Performing data splitting\")\n",
    "    X_train, X_val, X_test, Y_train, Y_val, Y_test = classifier_utils.get_xy_4_training(X,Y,seed)\n",
    "    print(X_train.shape,X_val.shape, X_test.shape)\n",
    "\n",
    "    gc.collect()\n",
    "    for exp in num_repetitions:\n",
    "        print(\"This is experiment: \", str(exp), \" with \", str(num_iq_seq))\n",
    "        \n",
    "        for model_type in models_to_train:\n",
    "\n",
    "            prefix_model = prefix+'_'+model_type+'_iq_samples_'+str(num_iq_seq)+'_num_classes_'+str(num_classes)+'_experiment_'+str(exp)+'_'+task\n",
    "\n",
    "            print('Starting data preparation and training for model ', model_type)\n",
    "            X_train_rs, X_val_rs, X_test_rs = classifier_utils.reshape_for_model(model_type, X_train, X_val, X_test)\n",
    "            print(X_train_rs.shape,X_val_rs.shape, X_test_rs.shape)\n",
    "            gc.collect()\n",
    "\n",
    "            \n",
    "            epochs = models_parameters[model_type]['max_epochs']\n",
    "            kernel_size= models_parameters[model_type]['kernel_size']\n",
    "            n_filters = models_parameters[model_type]['n_filters']\n",
    "            dropout = models_parameters[model_type]['dropout']\n",
    "            batch_size = models_parameters[model_type]['batch_size']\n",
    " \n",
    "            result, model = classifier_utils.create_and_train_model(model_type, num_iq_seq, num_classes, prefix_model, \n",
    "                                                                    X_train_rs, Y_train, X_val_rs, Y_val, \n",
    "                                                                    X_test_rs, Y_test, save=True, \n",
    "                                                                    epochs = epochs, n_filters=n_filters, \n",
    "                                                                    dropout=dropout, kernel_size=kernel_size, batch_size=batch_size)\n",
    "\n",
    "\n",
    "            epoch_times = model.get_time_per_epoch()\n",
    "            #mean_training_time_epoch = np.mean(epoch_times, axis=0)\n",
    "            #print(epoch_times, mean_training_time_epoch)\n",
    "\n",
    "            result['epoch_times']=epoch_times\n",
    "            experiment_results = classifier_utils.add_result(experiment_results, model_type, num_iq_seq, result, model_param, labels_string)\n",
    "\n",
    "\n",
    "            if exp ==1 & show_model:\n",
    "                model.get_model().summary()\n",
    "\n",
    "            #print(model_type, num_iq_seq, experiment_results)\n",
    "\n",
    "            #Save conf matrix\n",
    "            classifier_utils.compute_and_save_conf_matrix(model, X_test_rs, Y_test, labels_string, filename_prefix = prefix_model, precision = \"{:0.2f}\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = str(datenow)+'_'+task+'_results.json'\n",
    "classifier_utils.save_results_to_json(result_file_name,experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file_name) as json_file:\n",
    "    results_back = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(results_back, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
